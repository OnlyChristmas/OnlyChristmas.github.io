<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />





<!-- 添加DaoVioce -->



















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/imagess/玮.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/imagess/玮_32.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/imagess/玮_16.png?v=5.1.4">


  <link rel="mask-icon" href="/imagess/玮.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep-learning,Transformer,Attention,Transfer-learning,BERT,GNMT,Self-attention,Multi-head-attention,Positional-encoding,ELMo,Semi-supervised-learning,Pre-training" />





  <link rel="alternate" href="/atom.xml" title="Cape of Good Hope" type="application/atom+xml" />






<meta name="description" content="半个月前BERT横空出世，在数十个数据集上屠榜，一时风头无两。外加国内一些科技自媒体的“UC式”标题推波助澜，也给这篇文章博得了更多的关注。为了更好的理解BERT，我们需要先理解Attention和Transformer结构。然后可以集中精力从Transfer Learning的角度来比较ELMo，GPT，BERT这三篇文章的优劣异同。">
<meta name="keywords" content="Deep-learning,Transformer,Attention,Transfer-learning,BERT,GNMT,Self-attention,Multi-head-attention,Positional-encoding,ELMo,Semi-supervised-learning,Pre-training">
<meta property="og:type" content="article">
<meta property="og:title" content="BERT：From Transformer Architecture to Transfer Learning">
<meta property="og:url" content="https://imbowei.com/2018/10/27/From-transformer-architecture-to-transfer-learning/index.html">
<meta property="og:site_name" content="Cape of Good Hope">
<meta property="og:description" content="半个月前BERT横空出世，在数十个数据集上屠榜，一时风头无两。外加国内一些科技自媒体的“UC式”标题推波助澜，也给这篇文章博得了更多的关注。为了更好的理解BERT，我们需要先理解Attention和Transformer结构。然后可以集中精力从Transfer Learning的角度来比较ELMo，GPT，BERT这三篇文章的优劣异同。">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://i.loli.net/2018/11/05/5bdfb4b9098ff.png">
<meta property="og:image" content="https://i.loli.net/2018/11/05/5bdfb4ece6372.png">
<meta property="og:image" content="https://i.loli.net/2018/11/05/5bdfb68fe72f4.png">
<meta property="og:image" content="https://i.loli.net/2018/11/05/5be0216d8c44d.png">
<meta property="og:image" content="https://i.loli.net/2018/11/05/5be0262cb117e.png">
<meta property="og:image" content="https://i.loli.net/2018/11/06/5be0fe3f2715a.png">
<meta property="og:image" content="https://i.loli.net/2018/11/06/5be1434e29433.png">
<meta property="og:image" content="https://i.loli.net/2018/11/06/5be14b45cf031.png">
<meta property="og:image" content="https://i.loli.net/2018/11/06/5be14c3671643.png">
<meta property="og:image" content="https://i.loli.net/2018/11/06/5be14c6092d68.png">
<meta property="og:image" content="https://i.loli.net/2018/11/07/5be2e43bed325.png">
<meta property="og:image" content="https://i.loli.net/2018/11/06/5be197e531ba6.png">
<meta property="og:image" content="https://i.loli.net/2018/11/07/5be2ed16e4dcd.png">
<meta property="og:image" content="https://i.loli.net/2018/11/08/5be39de16d099.png">
<meta property="og:updated_time" content="2018-11-09T00:40:14.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BERT：From Transformer Architecture to Transfer Learning">
<meta name="twitter:description" content="半个月前BERT横空出世，在数十个数据集上屠榜，一时风头无两。外加国内一些科技自媒体的“UC式”标题推波助澜，也给这篇文章博得了更多的关注。为了更好的理解BERT，我们需要先理解Attention和Transformer结构。然后可以集中精力从Transfer Learning的角度来比较ELMo，GPT，BERT这三篇文章的优劣异同。">
<meta name="twitter:image" content="https://i.loli.net/2018/11/05/5bdfb4b9098ff.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: 'FHEEI2Y7D7',
      apiKey: '702daefe9c477cadc0f4905f578f5bf3',
      indexName: 'personal_blog',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"不好意思，木有'${query}'的搜索结果😂","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://imbowei.com/2018/10/27/From-transformer-architecture-to-transfer-learning/"/>






<!-- 网页加载条 -->
<script src="https://neveryu.github.io/js/src/pace.min.js"></script>



  <title>BERT：From Transformer Architecture to Transfer Learning | Cape of Good Hope</title>
  




<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-122082724-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->






</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Cape of Good Hope</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">I miss you</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-top">
          <a href="/top/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-signal"></i> <br />
            
            Top
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      

           <!-- 自定义High一下的功能 -->
           <!-- 现有歌单  1、#Lov3 #Ngẫu Hứng 2、骄傲的少年 3、博主原有-->
      <li class="menu-item"> <a title="把这个链接拖到你的工具栏中,任何网页都可以High" href='javascript:(
    /*
     * Copyright (C) 2016 Never_yu (Neveryu.github.io) <React.dong.yu@gmail.com>
     * Sina Weibo (http://weibo.com/Neveryu)
     *
     * Licensed under the Apache License, Version 2.0 (the "License");
     * you may not use this file except in compliance with the License.
     * You may obtain a copy of the License at
     *
     *      http://www.apache.org/licenses/LICENSE-2.0
     *
     * Unless required by applicable law or agreed to in writing, software
     * distributed under the License is distributed on an "AS IS" BASIS,
     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     * See the License for the specific language governing permissions and
     * limitations under the License.
     */




    function go() {

    var songs = [
         "http://www.170mv.com/kw/other.web.ri01.sycdn.kuwo.cn/resource/n3/43/85/4088203395.mp3",
         "http://www.170mv.com/kw/other.web.rm01.sycdn.kuwo.cn/resource/n3/10/77/3147640053.mp3",
         "http://7xoiki.com1.z0.glb.clouddn.com/Music-sunburst.mp3",
         ""
    ];


    function c() {
        var e = document.createElement("link");
        e.setAttribute("type", "text/css");
        e.setAttribute("rel", "stylesheet");
        e.setAttribute("href", f);
        e.setAttribute("class", l);
        document.body.appendChild(e)
    }

    function h() {
        var e = document.getElementsByClassName(l);
        for (var t = 0; t < e.length; t++) {
            document.body.removeChild(e[t])
        }
    }

    function p() {
        var e = document.createElement("div");
        e.setAttribute("class", a);
        document.body.appendChild(e);
        setTimeout(function() {
            document.body.removeChild(e)
        }, 100)
    }

    function d(e) {
        return {
            height : e.offsetHeight,
            width : e.offsetWidth
        }
    }

    function v(i) {
        var s = d(i);
        return s.height > e && s.height < n && s.width > t && s.width < r
    }

    function m(e) {
        var t = e;
        var n = 0;
        while (!!t) {
            n += t.offsetTop;
            t = t.offsetParent
        }
        return n
    }

    function g() {
        var e = document.documentElement;
        if (!!window.innerWidth) {
            return window.innerHeight
        } else if (e && !isNaN(e.clientHeight)) {
            return e.clientHeight
        }
        return 0
    }

    function y() {
        if (window.pageYOffset) {
            return window.pageYOffset
        }
        return Math.max(document.documentElement.scrollTop, document.body.scrollTop)
    }

    function E(e) {
        var t = m(e);
        return t >= w && t <= b + w
    }

    function S() {
        var e = document.getElementById("audio_element_id");
        if(e != null){
            var index = parseInt(e.getAttribute("curSongIndex"));
            if(index > songs.length - 2) {
                index = 0;
            } else {
                index++;
            }
            e.setAttribute("curSongIndex", index);
            N();
        }

        e.src = i;
        e.play()
    }

    function x(e) {
        e.className += " " + s + " " + o
    }

    function T(e) {
        e.className += " " + s + " " + u[Math.floor(Math.random() * u.length)]
    }

    function N() {
        var e = document.getElementsByClassName(s);
        var t = new RegExp("\\b" + s + "\\b");
        for (var n = 0; n < e.length; ) {
            e[n].className = e[n].className.replace(t, "")
        }
    }

    function initAudioEle() {
        var e = document.getElementById("audio_element_id");
        if(e === null){
            e = document.createElement("audio");
            e.setAttribute("class", l);
            e.setAttribute("curSongIndex", 0);
            e.id = "audio_element_id";
            e.loop = false;
            e.bgcolor = 0;
            e.addEventListener("canplay", function() {
            setTimeout(function() {
                x(k)
            }, 500);
            setTimeout(function() {
                N();
                p();
                for (var e = 0; e < O.length; e++) {
                    T(O[e])
                }
            }, 15500)
        }, true);
        e.addEventListener("ended", function() {
            N();
            h();
            go();
        }, true);
        e.innerHTML = " <p>If you are reading this, it is because your browser does not support the audio element. We recommend that you get a new browser.</p> <p>";
        document.body.appendChild(e);
        }
    }

    initAudioEle();
    var e = 30;
    var t = 30;
    var n = 350;
    var r = 350;

    var curSongIndex = parseInt(document.getElementById("audio_element_id").getAttribute("curSongIndex"));
    var i = songs[curSongIndex];

    var s = "mw-harlem_shake_me";
    var o = "im_first";
    var u = ["im_drunk", "im_baked", "im_trippin", "im_blown"];
    var a = "mw-strobe_light";

    /* harlem-shake-style.css，替换成你的位置，也可以直接使用：//s3.amazonaws.com/moovweb-marketing/playground/harlem-shake-style.css */
    var f = "//s3.amazonaws.com/moovweb-marketing/playground/harlem-shake-style.css";

    var l = "mw_added_css";
    var b = g();
    var w = y();
    var C = document.getElementsByTagName("*");
    var k = null;
    for (var L = 0; L < C.length; L++) {
        var A = C[L];
        if (v(A)) {
            if (E(A)) {
                k = A;
                break
            }
        }
    }
    if (A === null) {
        console.warn("Could not find a node of the right size. Please try a different page.");
        return
    }
    c();
    S();
    var O = [];
    for (var L = 0; L < C.length; L++) {
        var A = C[L];
        if (v(A)) {
            O.push(A)
        }
    }
    })()'><i class="menu-item-icon fa fa-music fa-fw"></i>Interesting</a> </li>
      <!-- end High一下 -->

    </ul>

  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://imbowei.com/2018/10/27/From-transformer-architecture-to-transfer-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Christmas">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/christmas_ID_round.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cape of Good Hope">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">BERT：From Transformer Architecture to Transfer Learning</h2>
        

        <div class="post-meta">
          <span class="post-time">
            

              <!-- 注释掉图片 --->
        <!--  <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span> -->


              
                <span class="post-meta-item-text">Posted on:</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-27T20:07:40+08:00">
                2018-10-27
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            

              <!-- 注释掉图片 --->
       <!--   <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>  -->


              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2018-11-09T08:40:14+08:00">
                2018-11-09
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            

            <!-- 注释掉图片 -->
        <!--  <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span> -->

              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep-learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/Pre-training/" itemprop="url" rel="index">
                    <span itemprop="name">Pre-training</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2018/10/27/From-transformer-architecture-to-transfer-learning/" class="leancloud_visitors" data-flag-title="BERT：From Transformer Architecture to Transfer Learning">
               <span class="post-meta-divider">|</span>

               <!-- 注释掉了图片 -->
          <!-- <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>   -->

               
                 <span class="post-meta-item-text">Heat&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
                 <span>℃</span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                

                <!-- 注释掉图片 -->
         <!--   <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>  -->

                
                  <span class="post-meta-item-text">Words&#58;</span>
                
                <span title="Words">
                  5,582
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>半个月前BERT横空出世，在数十个数据集上屠榜，一时风头无两。外加国内一些科技自媒体的“UC式”标题推波助澜，也给这篇文章博得了更多的关注。为了更好的理解BERT，我们需要先理解Attention和Transformer结构。然后可以集中精力从Transfer Learning的角度来比较ELMo，GPT，BERT这三篇文章的优劣异同。 </p>
<a id="more"></a>
<hr>
<blockquote class="blockquote-center"><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2></blockquote>
<p>什么是注意力机制？可以想象这样一个画面，当电视机上有非常吸引我们的画面时，我们的视野之中除了电视画面，眼中所看到的屋子中的其他部分仿佛都变得模糊了。甚至妈妈在你的面前从一侧穿行到另外一侧，你都毫无察觉。</p>
<p>有所关注，有所忽略——这便是注意力机制。</p>
<p>从生物进化的角度上来讲，这是十分合理的“节能减排”，我们的大脑将资源集中分到了我们最关心的事物上面，让我们免受其他事务的干扰。</p>
<p>Attention机制最早应用于<a href="http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf" target="_blank" rel="noopener">视觉领域的分类问题</a>，而为了更好的取得词向量表达， <a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener">Bengio</a> 在2014年引入了Attention机制，这极大的提升了NMT方法的性能。尽管仍存在训练时间长，OOV，可解释性差等问题，但某些性能指标上已经媲美SMT系统，并大大减少了形态学和句法错误，提升了翻译的流畅性。这篇论文也直接影响Google在2016年用GNMT系统替换掉了上线十年之久的PBMT系统，不仅将Attention机制推到闪耀的聚光灯之下，也彻底将SMT方法推下了机器翻译的神坛，开启了NMT方法狂飙的时代。</p>
<p><img src="https://i.loli.net/2018/11/05/5bdfb4b9098ff.png" alt="NMT系统的发展趋势"></p>
<p>以2013年Nal Kalchbrenner 和 Phil Blunsom的 <a href="http://www.aclweb.org/anthology/D13-1176" target="_blank" rel="noopener">Recurrent Continuous Translation Models</a> 论文为标志，神经网络机器翻译诞生。该文整体采用编码器——解码器框架，用CNN将源文本编码成特定的向量，再用一个RNN作为解码器将该向量转化为目标语言。但由于梯度消失/爆炸问题的存在， 长距离依存问题制约模型的表现。 为了缓解这一问题，<a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sutskever et al.</a> 和 <a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener">Cho et al.</a> 引入了LSTM。</p>
<p><img src="https://i.loli.net/2018/11/05/5bdfb4ece6372.png" alt="GNMT系统效果图"></p>
<h3 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h3><p>以上就是Attention机制在NLP领域的应用背景和重要意义，下面我们来详细理解它的作用机制。</p>
<h4 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h4><p>普通的LSTM是采取的是序列编码的方式来获取信息，这样的一大好处就是在编码的时候可以得到词语的远距离依赖信息。如下图所示：我们在编码“力量”的时候，可以知道“知识”是它的来源。</p>
<p><img src="https://i.loli.net/2018/11/05/5bdfb68fe72f4.png" alt="A Example Seq2seq"></p>
<p>下面我们来看一个形象化的基于sequence-to-sequence框架的NMT模型。先看编码器，除了最初状态之外，后续每个状态都接收前一个状态的编码信息以及当前位置的源句子词语信息，整个源句子顺次编码。在编码完源句子最后一个词的信息之后，才执行解码操作。同样的，解码结构也是顺序执行的。视频中给出的过程应该是测试过程，前一个位置没有传递给下一个位置真实的解码词语参考，而在训练过程中是要给出的。</p>
<iframe width="100%" height="540" align="middle" src="http://imbowei.com/imagess/Attention/seq2seq_6.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<p>Seq2seq构架的劣势也不难发现：</p>
<ol>
<li>如果单向RNN编码，前面的词语在先行编码的时候没有办法得到后面词语的信息； 一个形象的例子，</li>
</ol>
<div class="note info"><p>“I arrived at the <span id="inline-purple">bank</span> after crossing the <span id="inline-blue">….</span> ”</p></div>

<p>我们在不知道后面省略的单词是 <span id="inline-blue">river</span> or <span id="inline-blue">road</span> 的前提下，该如果编码单词 <span id="inline-purple">bank</span>  的真实含义呢？</p>
<ol>
<li>信息损失：如果句子很长，两次词语之间相隔比较远，信息传递过程中的损失比较大；</li>
<li>信息糅杂：最后一个状态中理论上包含前面所有的信息，所有信息都杂糅在一起不好区分；</li>
<li>顺序编码本质上是一个马尔可夫决策过程，无法很好的得到全局信息；</li>
<li><strong>无法并行计算</strong>，只有当所有词语都编码结束的时候才可以开始解码，系统的训练速度很慢；</li>
</ol>
<h4 id="Seq2seq-Attention"><a href="#Seq2seq-Attention" class="headerlink" title="Seq2seq+Attention"></a>Seq2seq+Attention</h4><p>加上Attention Mechainism的seq2seq结构解决了上述 Long Range Dependence弊端。与seq2seq过程对比，我们观察下面一个视频。</p>
<iframe width="100%" height="540" align="middle" src="http://imbowei.com/imagess/Attention/seq2seq_7.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<ol>
<li><p>首先避免信息缺失问题，将Encoder每个时刻的状态都传递给Decoder，而不是只传递Encoder最后一个时刻的状态。最大程度上保留源句子的信息。</p>
</li>
<li><p>其次，为了避免信息冗余，我们如何只在每个时刻关注我们最关心的部分呢？用一个动态的权重向量（注意力分布），根据其与所有Encoder的隐藏状态信息的相似性大小进行加权求和。加权计算的过程，如下视频所示。</p>
<iframe width="100%" height="540" align="middle" src="http://imbowei.com/imagess/Attention/attention_process.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</li>
<li><p>最后，解码时，每一时刻的注意力分布都由这一时刻的解码hidden states来决定。这一时刻的hidden states的输出和注意力机制加权过的向量拼接后归一化，概率最大的一维所对应的词语就是我们这一时刻的翻译结果。过程如下动画所示。</p>
</li>
</ol>
<iframe width="100%" height="540" align="middle" src="http://imbowei.com/imagess/Attention/attention_tensor_dance.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<p>经过上述Attention结构上的剖析，我们再次来总结什么是 Attention 机制？</p>
<p>在我看来，Attention 其实就是一种用在Seq2seq构架上的加权词对其模型。起名为注意力机制，一方面是为了可以从大众容易理解的方式更好的解释模型设计的合理性；另一方面，我认为也是为了更容易完成学术投稿吧。Just a writing trick！</p>
<blockquote class="blockquote-center"><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2></blockquote>
<p>在GNMT掀起神经网络机器翻译的热潮之后，Amazon、Microsoft、Facebook、百度、网易有道、腾讯 、搜狗、讯飞、阿里巴巴等公司都迅速跟进。其中Facebook以CNN为基础的NMT模型不但效果超过了Google的GNMT，而且在训练速度上也比前者大幅提升九倍。</p>
<p><img src="https://i.loli.net/2018/11/05/5be0216d8c44d.png" alt="2017年NMT模型效果对比"></p>
<p>Google可不能容忍这个风头被抢，作为回应，2017.06谷歌发布了一个完全以注意力机制为基础的NMT模型，也就是我们常说的Transformer。整体构架如下所示，观察模型结构，正如作者所说，Attention is all you need。</p>
<p><img src="https://i.loli.net/2018/11/05/5be0262cb117e.png" alt="Transformer Architecture"></p>
<h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>对于Transformer构架而言，最核心的部分莫过于Self-attention。如下图所示，每个词语的初始向量乘以三个权重矩阵得到q、k、v三个向量。重新编码每个词语的时候，所有词语的k向量分别与该词语的q向量相乘，然后除以模型维数的开方，最终权重归一化成values向量的权重值。这样的操作，是根据词语向量之间的相似度来完成的。也就是说，每个词语的新向量必然还是会以自身的原本的信息为主要组成部分，只是在此基础上无距离差别的补充了与其他词语的相关性信息。相当于，我们通过整个句子中所有词语之间的相互关系获取了每个词语在句子中的重要性程度，有了更加丰富的语义信息。另外，所有词语获得新向量的过程是可以并行计算的（矩阵运算），即使模型的参数量巨大，在GPU的加持下也获得了远远高于LSTM的训练速度。</p>
<p><img src="https://i.loli.net/2018/11/06/5be0fe3f2715a.png" alt="Self-Attention"></p>
<script type="math/tex; mode=display">
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><h3 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h3><p>上述Self-Attention虽然构思巧妙，但是只能说是一个精妙的词袋模型，并没有捕捉到任何次序信息。很不幸，这对翻译任务是致命的。为了能让这个简单的结构很好的工作，这个时候作者提供了一个额外的词向量去补充词语之间的位置关系，缓解这一缺陷。公式如下，采用的是三角函数的形式，好处是在测试的时候如果遇到超出我们训练长度限制的句子，模型也可以进行位置编码。</p>
<script type="math/tex; mode=display">
PE_(pos,2i)=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \\
PE_(pos,2i+1)=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})</script><p><img src="https://i.loli.net/2018/11/06/5be1434e29433.png" alt="Position Embedding"></p>
<p>然而，Position Embedding对于本身模型不能捕捉位置信息，只是起到了一个弥补的作用，并不能从根本上解决模型设计上的缺陷。在实验结果上也可以察觉出一些端倪，为什么同一语系的双语翻译的BLUE数值表现会比不同语系之间的双语翻译效果要好呢？我猜测是同一语系不同语言之间的语序差别较小，而不同语系语言之间的语序差别较大。甚至说，以BLUE作为翻译效果的自动评价指标对于Transformer模型也是占便宜的，因为语言模型本身对于语序正确的要求并没有很高。</p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>Multi-Head Attention这个概念是在这篇论文中第一次被提及，然而其实际操作并不罕见。其实就是将Self-Attention这一个过程随机初始化8次，相当于映射到不同的子空间，然后拼接起来并乘以权重向量产生输出层。 相当于我们从多种角度来理解同一个句子，以求更加完备的语义表达。</p>
<p><img src="https://i.loli.net/2018/11/06/5be14b45cf031.png" alt="Multi-Head Attention"></p>
<script type="math/tex; mode=display">
MultiHead(Q,K,V)=Concat(head_1,...,headf_h)W^o \\
where\ head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script><p>最后我们来观察一下Multi-Head Attention的效果，已经其背后的含义。下面的第一张图片只展示了两次的Multi-Head Attention，我们还可以尝试来解释。黄色的注意力机制敏锐了步骤到了<strong>it</strong>这个代词所指代的对象，是<strong>animal</strong>；而绿色的注意力机制貌似是错误的，因为直接指向了一个动词<strong>tire</strong>。但细细琢磨，这似乎也有一定的道理，因为动词<strong>tire</strong>是服务于主语<strong>animal</strong>，而<strong>it</strong>在这里恰好指代的就是主语<strong>animal</strong>。</p>
<p><img src="https://i.loli.net/2018/11/06/5be14c3671643.png" alt="Multi-Head_1"></p>
<p>由上面的观察分析可以发现，多重的注意力机制确实可以捕捉到许多句子中隐含的语义细节，得到更好的句子标示。当这种语义表示维度过高时，我们往往难以解释其真实含义。但是，确实效果还不错。</p>
<p><img src="https://i.loli.net/2018/11/06/5be14c6092d68.png" alt="Multi-Head_2"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>总的来说，Transformer构架的核心思想是计算一句话中每个词对于这句话中其它所有词的相互关系，然后认为这些词与词之间的相互关系在一定程度上反应了这句话中不同词之间的关联性以及它们的重要程度。再利用这些相互关系来调整每个词的重要性（权重）就可以获得每个词新的表达。这个新的表征不但蕴含了该词本身，还蕴含了其他词与这个词的关系，因此和单纯的词向量相比是一个更加全局的表达。Transformer通过对输入的文本不断进行这样的注意力机制层和普通的非线性层交叠来得到最终的文本表达。拥有一个文本信息更加充分全面的表达，得到更好的BLUE数值表现也就顺理成章了。</p>
<p>根据上述介绍的模型结构，并且结合Transformer结构提出的背景，不难看出其浓重的CNN气息。具体的有以下几个方面：</p>
<ol>
<li>Attention是不是像一个没有感受野限制的CNN？所有单词之间的距离为一，一步到位的获得全局信息，免去的CNN的堆叠操作。</li>
<li>Multi-Head Attention多次运算进行拼接操作，对比CNN中的多个卷积核，是否感觉似曾相识？</li>
<li>同CNN一样，需要一个Position Embedding 来辅助获得位置信息。只不过这个更为“泛化的CNN”对Position Embedding的依赖更加严重。</li>
</ol>
<p>尽管作者的writing trick让人有些许反感，但这并不失为一篇好论文。</p>
<ul>
<li><p>首先，模型构架大道至简。感概Google工程师扎实的炼丹功底，如此简单的方法都可调出STOA。</p>
<ul>
<li>Attention 并不简简单单可以完成“词对齐”工作，而且可以完成序列到序列的转换。</li>
</ul>
</li>
<li><p>其次，写作清晰，整篇论文读起来很清爽，没有故弄玄虚的堆砌公式。（可惜没有Ablation Test，不能确定Transformer构架各个部分的作用效果）</p>
</li>
<li><p>最后，运用些许的写作技巧来提升自己工作的影响力这本无可厚非（这个名字确实太吸引人去阅读他们的论文了）。也是我需要尽快提高的地方。。</p>
</li>
</ul>
<blockquote class="blockquote-center"><h2 id="Transfer"><a href="#Transfer" class="headerlink" title="Transfer"></a>Transfer</h2></blockquote>
<p>源于Google扎实的工作（夸张的论文名字），Transformer 坐上了NMT模型的王座，更是在NLP领域一时风头无两，在众多任务上都取得了很好的表现。当其应用到迁移学习领域的时候，就创造出了现在Google另一个红透半边天的工作——BERT。</p>
<h3 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h3><p>2018初的NAACL上，AllenNLP祭出了大杀器ELMo。而Google将自身的 Transformer 构架用在迁移学习领域，相信少不了受到了这篇 <a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">Deep contextualized word representations</a> 文章的启发。</p>
<p>在EMLo出现之前，不论时研究者习惯性地用word2vec去初始化，还是尝试性的用CNN、RNN等网络结构探索字符级别的文本向量。然而，无论再怎么折腾，怎么细化挖掘信息的级别。都有共同的一点，Embedding没有考虑到<span id="inline-blue">语境中的上下文信息</span> ，总是一成不变的，没有表征一词多义的能力。而EMLo拿出了极具说服力的表现告诉大家，是时候该放弃古老的word2vec了。。。</p>
<p>这篇论文有以下几个关键点：</p>
<ul>
<li>CNN提取字符（中文可以考虑拼音或者笔画）级别信息；</li>
<li>两个单向LSTM，<strong>loss相互独立</strong>；</li>
<li>浅层<strong>语法</strong>，深层<strong>语义</strong>；</li>
<li>层级输出线性加权，可以根据任务需要自训练不同的权重；</li>
</ul>
<h3 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h3><p>趁热打铁，AllenNLP 又从迁移学习 Fine-tuning的角度发布了另一个重要工作——Improving Language Understanding by Generative Pre-training(GPT)。这个工作也直接引导出了Google的重磅论文BERT。如果说ELMo的野心只是想要取代Word2vec，成为NLP领域文本输入的标准操作。那GPT显然野心更大，其试图提出一个通用的框架网络，想要成为类似于图像领域的ResNet那样的神经网络骨架，以扭转现在研究task-specific方法愈演愈烈的风气。拯救近期国内外一些公司在某些固定的数据集上穷尽奇淫巧技去调试出一个个过拟合模型，抢占所谓的STOA。</p>
<p>GPT提出了一个两阶段的模型。第一阶段，在大规模的无监督文本上训练一个单向的Transformer模型；第二阶段，只需要根据不同的任务要求，仅需要少量的特定标注数据进行调优训练，即可获得若干任务数据集上STOA级别的表现。这样的迁移学习方法，极大的节省了为不同的特定任务进行人工标注数据的高昂成本需求。</p>
<p>GPT已经取得了很令人实验结果，有以下几个点值得我们多加关注：</p>
<ol>
<li>继续增加生语料，模型效果是否会继续提升？</li>
<li>泛化能力不够强，如果精细化调优技巧（类似：ULMFiT每一层设置不同的学习率？），会不会在适应各个任务的数据集上有更好的表现？</li>
<li>5GB语料，需要8块GPU训练一个月，如何合理的拓展出降低计算需求的方法？</li>
<li>现用的训练生语料，可能由于其自身局限性，导致模型认知世界产生偏差。</li>
</ol>
<h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p>顺着GPT的思路，今年10月BERT的横空出世，无疑将2018年NLP领域无监督学习的热潮推向了一个新的高度! 先来看看它在GLUE leaderboard屠榜的盛况。</p>
<p><img src="https://i.loli.net/2018/11/07/5be2e43bed325.png" alt="GLUE leaderboard"></p>
<p>为什么BERT的性能如此只有优越呢？由于它在以下的几个方面在GPT的基础上有了进一步的突破：</p>
<h4 id="Deep-model"><a href="#Deep-model" class="headerlink" title="Deep model"></a>Deep model</h4><p>jacob 以极强的工程能力成功驾驭了一个深达24层、每层1024个神经元、并且Multi-dead数为16、总参数340M的巨大Transformer模型，再一次向我们证明了对于深度学习来说，深度&gt;宽度这一重要规律。当然，也体现了Transformer模型具有良好的稳定性这一重要优点，因为其本身就含有各种Normalization。</p>
<h4 id="Masked-LM"><a href="#Masked-LM" class="headerlink" title="Masked LM"></a>Masked LM</h4><p><img src="https://i.loli.net/2018/11/06/5be197e531ba6.png" alt="Bert"></p>
<p>观察上图，相比于GPT，BERT在训练上解决了无法使用双向Transformer的窘境。相比于ELMo，两个方向上的loss结合在一起，而不是相互独立，更大程度上释放了Transformer构架的信息采集能力。不难理解为什么双向的Transformer效果要更好，就像我们学习英语做完形填空。我们是只看半句填的准呢？还是前后半句都阅读完填的准呢？相信这不难回答。 双向训练具体是怎么做到的呢？观察下图，当模型层数加深的时候，在每个位置上已经有了原本这个位置上的词的信息，这对于词的预测任务来说无异于作弊，显然是不可行的一种方案。</p>
<p><img src="https://i.loli.net/2018/11/07/5be2ed16e4dcd.png" alt="双向Transformer的数据窥探问题"></p>
<p>那么怎么办呢？直接删除需要预测的词语吗？这会是原本的句子失去顺序信息，并且丢掉了这一个词的原本信息，显然不可取。替换为一个随机的词吗？但这会使模型难以收敛，也不可以。最后作者选择只将10%的词语进行随机处理（相当于噪声），而将80%的词用“mask”标记来遮盖，让模型通过这个编辑来学习该位置的填词。然而下游任务中显然不会存在这个“mask”标记，所以最后另外10%的词语我们保持原有这个原有正确的词语。虽然通过这样一个训练技巧，使得双向的Transformer可以正常的工作了。但是由于加入了大量的噪声，模型的收敛仍有待提升。</p>
<h4 id="Jointly-Pre-Train"><a href="#Jointly-Pre-Train" class="headerlink" title="Jointly Pre-Train"></a>Jointly Pre-Train</h4><p>由于很多的NLP下游任务涉及到句子之间关系的理解，例如：Question Answering (QA) 、Natural Language Inference (NLI)等等。作者在原有的loss函数后面新添加了一项关于句子间关系理解的loss，来进行联合训练。如下所示，这仅仅是一个简单的二分类问题。jacob将此处的训练模式与下游任务相统一，都是将两个句子一起作为输入（中间有间隔符[SEP]）。用无监督语料训练的时候，将一半的句子下一句进行随机句子采样，作为负例“NotNext”；而其余一半则直接给出真正的后句，作为正例“IsNext”。</p>
<blockquote>
<p> Input = [CLS] the man went to [MASK] store [SEP]   he bought a gallon [MASK] milk [SEP]</p>
<p>Label = IsNext</p>
<p>Input = [CLS] the man [MASK] to the store [SEP]   penguin [MASK] are flight ##less birds [SEP]</p>
<p>Label = NotNext</p>
</blockquote>
<h4 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h4><p>昨天，<a href="https://github.com/google-research/bert/blob/master/multilingual.md" target="_blank" rel="noopener">中文定制版model</a>也可以在github上获取了。</p>
<h5 id="Position-Embedding-1"><a href="#Position-Embedding-1" class="headerlink" title="Position Embedding"></a>Position Embedding</h5><p>BERT并没有采取GNMT那篇论文中用三角函数来表达句子中词语位置的方法，而是直接设置句子的固定长度去训练Position Embedding，在每个词的位置随机初始化词向量，经过训练，将Position Embedding与Token Embedding、以及模型训练得到的Segment Embeddings 直接相加即可食用。如下图所示。</p>
<p><img src="https://i.loli.net/2018/11/08/5be39de16d099.png" alt="Embeddings"></p>
<h5 id="feature-based？"><a href="#feature-based？" class="headerlink" title="feature-based？"></a>feature-based？</h5><p>在倒数第二页的一栏，作者实验表明，BERT不仅仅是一个Fine-tuning迁移学习方法，还可以是一个Feature-based迁移学习方法，就像ELMo和Wordsvec。jacob用CoNLL-2003 NER数据集做了实验，feature-based（合并最后四层的输出）版本的BERT仅仅比fine-tuning版本的BERT低0.3的准确率，再次心疼一下帅不过三秒的ElMo和GPT。</p>
<h5 id="More-Data"><a href="#More-Data" class="headerlink" title="More Data"></a>More Data</h5><p>相比于GPT模型5GB、800M words 的训练数据，BERT更是用了惊人的3,200M words的训练数据。联想到GPT论文中提到，如果有更多的训练语料，该模型的效果还会进一步提升。我觉得在BERT论文中应该有一个关于Data size的Ablation study，然而令我失望的是我并没有找到。</p>
<div class="note danger no-icon"><p>在BERT巨大的提升背后，我想知道模型上的创新和训练数据数量上的增加，谁贡献更大一点呢？jacob似乎避而不谈。</p></div>

<blockquote class="blockquote-center"><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2></blockquote>
<p>总的来说，jacob凭借着高超的工程技能，海量的训练数据，以及强大的算力，完成了一篇里程碑级别论文。它的出现在NLP社区产生了重要的影响，并在以下的几方面给我们带来了许多的惊喜和思考：</p>
<ul>
<li>深度、双向的transformer构架具有强大的数据表示能力以及良好的稳定性；</li>
<li>Jointly Pre-Train 对于训练的帮助；</li>
<li>Pre-Training 的这把火会以何种形式烧到 NLG ？</li>
<li>One for All：手握大量资源的公司越来越倾向于发布一个简单粗暴的通用模型去解决一众问题，科研院校该何去何从？</li>
<li>Few-shot Learning：大数据的学习模式真的是机器学习的归宿吗？科研院校是否该从其他角度发力，来探索ML的未来？</li>
</ul>
<blockquote class="blockquote-center"><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2></blockquote>
<ul>
<li><a href="http://delivery.acm.org/10.1145/980000/972474/p263-brown.pdf?ip=115.27.204.47&amp;id=972474&amp;acc=OPEN&amp;key=BF85BBA5741FDC6E%2EAC95BC9DA5A3FA7E%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;__acm__=1541382525_61f1fb6c7dbdb82f34516a5c0f6c8545" target="_blank" rel="noopener">The Mathematics of Statistical Machine Translation: Parameter Estimation</a></li>
<li><a href="http://www.aclweb.org/anthology/D13-1176" target="_blank" rel="noopener">Recurrent Continuous Translation Models</a></li>
<li><a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sequence to sequence learning with neural networks</a></li>
<li><a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li>
<li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">Harvard NLP</a></li>
<li><a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">Tensor2tensor</a></li>
<li><a href="https://gluebenchmark.com/leaderboard" target="_blank" rel="noopener">GLUE Leaderboard</a></li>
<li><a href="http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf" target="_blank" rel="noopener">Recurrent Models of Visual Attention</a></li>
<li><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation</a></li>
<li><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">Attention is All you Need</a></li>
<li><a href="http://www.aclweb.org/anthology/P18-1031" target="_blank" rel="noopener">Universal Language Model Fine-tuning for Text Classification</a></li>
<li><a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">Deep Contextualized Word Representations</a></li>
<li><a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-training</a></li>
<li><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
<li><a href="https://arxiv.org/pdf/1706.05137.pdf" target="_blank" rel="noopener">One Model to Learn Them All</a></li>
<li><a href="https://arxiv.org/pdf/1706.03059.pdf" target="_blank" rel="noopener">Depthwise Separable Convolutions for Neural Machine Translation</a></li>
<li><a href="https://content.sciendo.com/view/journals/pralin/110/1/article-p43.xml" target="_blank" rel="noopener">Training Tips for the Transformer Model</a></li>
<li><a href="https://arxiv.org/pdf/1803.02155.pdf" target="_blank" rel="noopener">Self-Attention with Relative Position Representations</a></li>
<li><a href="https://arxiv.org/pdf/1804.04235.pdf" target="_blank" rel="noopener">Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</a></li>
<li><a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shif</a></li>
<li><a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="noopener">Layer Normalization</a></li>
<li><a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="noopener">Semi-supervised Sequence Learning</a></li>
</ul>

      
    </div>
    
    
    


    <div>
      
        <div>
    
        <blockquote class="blockquote-center">
        <div style="text-align:center;color: #FF5733;font-size:24px;">-------------本文结束 <i class="fa fa-heart"></i> 感谢您的时间-------------</div>
        </blockquote>
    
</div>
      
    </div>


    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div></div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/Wachat_money.png" alt="Christmas WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay_money.png" alt="Christmas Alipay"/>
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    Christmas
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="https://imbowei.com/2018/10/27/From-transformer-architecture-to-transfer-learning/" title="BERT：From Transformer Architecture to Transfer Learning">https://imbowei.com/2018/10/27/From-transformer-architecture-to-transfer-learning/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-learning/" <i class="fa fa-tag"></i> Deep-learning</a>
          
            <a href="/tags/Transformer/" <i class="fa fa-tag"></i> Transformer</a>
          
            <a href="/tags/Attention/" <i class="fa fa-tag"></i> Attention</a>
          
            <a href="/tags/Transfer-learning/" <i class="fa fa-tag"></i> Transfer-learning</a>
          
            <a href="/tags/BERT/" <i class="fa fa-tag"></i> BERT</a>
          
            <a href="/tags/GNMT/" <i class="fa fa-tag"></i> GNMT</a>
          
            <a href="/tags/Self-attention/" <i class="fa fa-tag"></i> Self-attention</a>
          
            <a href="/tags/Multi-head-attention/" <i class="fa fa-tag"></i> Multi-head-attention</a>
          
            <a href="/tags/Positional-encoding/" <i class="fa fa-tag"></i> Positional-encoding</a>
          
            <a href="/tags/ELMo/" <i class="fa fa-tag"></i> ELMo</a>
          
            <a href="/tags/Semi-supervised-learning/" <i class="fa fa-tag"></i> Semi-supervised-learning</a>
          
            <a href="/tags/Pre-training/" <i class="fa fa-tag"></i> Pre-training</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/09/29/How-to-write-a-great-paper/" rel="next" title="如何写好学术论文？">
                <i class="fa fa-chevron-left"></i> 如何写好学术论文？
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/11/16/an-unknown-person-is-not-unknown/" rel="prev" title="无名之辈不无名（A Cool Fish）">
                无名之辈不无名（A Cool Fish） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "1",
        "bdMiniList": false,
        "bdPic": ""
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      },
      "slide": {
        "bdImg": "5",
        "bdPos": "left",
        "bdTop": "100"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/christmas_ID_round.png"
                alt="Christmas" />
            
              <p class="site-author-name" itemprop="name">Christmas</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">26</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">69</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/OnlyChristmas" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i></a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:m_christmas@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i></a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://imbowei.com/" title="(●ˇ∀ˇ●)" target="_blank">(●ˇ∀ˇ●)</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention"><span class="nav-number">1.</span> <span class="nav-text">Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention-Mechanism"><span class="nav-number">1.1.</span> <span class="nav-text">Attention Mechanism</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Seq2seq"><span class="nav-number">1.1.1.</span> <span class="nav-text">Seq2seq</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Seq2seq-Attention"><span class="nav-number">1.1.2.</span> <span class="nav-text">Seq2seq+Attention</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer"><span class="nav-number">2.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-Attention"><span class="nav-number">2.1.</span> <span class="nav-text">Self-Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Position-Embedding"><span class="nav-number">2.2.</span> <span class="nav-text">Position Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Head-Attention"><span class="nav-number">2.3.</span> <span class="nav-text">Multi-Head Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Conclusion"><span class="nav-number">2.4.</span> <span class="nav-text">Conclusion</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transfer"><span class="nav-number">3.</span> <span class="nav-text">Transfer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ELMo"><span class="nav-number">3.1.</span> <span class="nav-text">ELMo</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GPT"><span class="nav-number">3.2.</span> <span class="nav-text">GPT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BERT"><span class="nav-number">3.3.</span> <span class="nav-text">BERT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Deep-model"><span class="nav-number">3.3.1.</span> <span class="nav-text">Deep model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Masked-LM"><span class="nav-number">3.3.2.</span> <span class="nav-text">Masked LM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Jointly-Pre-Train"><span class="nav-number">3.3.3.</span> <span class="nav-text">Jointly Pre-Train</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Others"><span class="nav-number">3.3.4.</span> <span class="nav-text">Others</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Position-Embedding-1"><span class="nav-number">3.3.4.1.</span> <span class="nav-text">Position Embedding</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#feature-based？"><span class="nav-number">3.3.4.2.</span> <span class="nav-text">feature-based？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#More-Data"><span class="nav-number">3.3.4.3.</span> <span class="nav-text">More Data</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">4.</span> <span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">5.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        
<div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="heart">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Christmas</span>

  
    <span class="post-meta-divider">|</span>

    <span class="post-meta-item-icon">
      <i class="fa fa-thumbs-up"></i>
    </span>


    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">46.0k</span>
  
</div>












  <div class="footer-custom">Hosted by <a href="https://pages.coding.me" >Coding Pages</a> and <a href="https://pages.github.com">GitHub Pages</a></div>





<span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("07/06/2018 11:11:00");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);


</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 你是第
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      个小伙伴
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      人次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  







<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    
      <script type="text/javascript">
      function renderGitment(){
        var gitment = new Gitmint({
            id: '1540642060000' , 
            owner: 'OnlyChristmas',
            repo: 'OnlyChristmas.github.io',
            
            lang: "" || navigator.language || navigator.systemLanguage || navigator.userLanguage,
            
            oauth: {
            
            
                client_secret: '4ff9a983c2a12891885a77a529cb04ce17400d46',
            
                client_id: '8fc5c130c7429cb7d9aa'
            }});
        gitment.render('gitment-container');
      }

      
      renderGitment();
      
      </script>
    







  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.4"></script>



  

  

  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("CYiT0KdkmU0LEo3kFuLb4TPP-gzGzoHsz", "6RIqigijTviGwWrSUQgaVMP7");</script>
  <script>

    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.4"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.4"></script>


  


  <!-- 页面点击小红心 -->
 <!--  <script type="text/javascript" src="/js/src/love.js"></script> -->


  <!-- 背景动画(不知道为什么，不起作用) -->
  <!-- <script type="text/javascript" src="/js/src/particle.js"></script> -->


  <!-- 页面点击烟花爆炸 -->
  
   <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas>
   <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
   <script type="text/javascript" src="/js/src/fireworks.js"></script>
  


  <!-- 加载加速 -->
  <script type="text/javascript" src= "/js/src/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>InstantClick.init();</script>

  <!--崩溃欺骗-->
  <!-- <script type="text/javascript" src="/js/src/custom_title.js"></script> -->


  <!-- 代码块复制功能 -->
  <script type="text/javascript" src="/js/src/clipboard.min.js"></script>
  <script type="text/javascript" src="/js/src/clipboard-use.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

<script async>window.onload=function(){var a=document.createElement('script'),b=document.getElementsByTagName('script')[0];a.type='text/javascript',a.async=!0,a.src='/sw-register.js?v='+Date.now(),b.parentNode.insertBefore(a,b)};</script></body></html>